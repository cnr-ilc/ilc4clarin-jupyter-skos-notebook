{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad632f86-f42c-4295-8861-daa52d34e6f1",
   "metadata": {},
   "source": [
    "# **LLOD Tutorial - REALITER Lexicons**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed616c01-1274-4ba4-aea5-02424d0545c9",
   "metadata": {},
   "source": [
    "## Creating a Lexicon in SKOS from Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f9d0e-7a23-4154-8ef7-6ecda93c4bf6",
   "metadata": {},
   "source": [
    "### **Introduction:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686a2fb-7974-409f-b92c-2eb9ba318e23",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the creation of a lexicon using the Simple Knowledge Organization System (SKOS) format. Specifically, we will focus on how to generate a lexicon based on structured data, utilizing the lexicons developed within the REALITER project. This guide will cover the transformation process of linguistic data into the SKOS format, making it interoperable with Linked Data principles and the Linguistic Linked Open Data (LLOD) paradigm.\n",
    "\n",
    "Once the lexicon is created, it will be hosted on **Skosmos** and accessible at the following address: [https://vocabs.ilc4clarin.ilc.cnr.it/skosmos/](https://vocabs.ilc4clarin.ilc.cnr.it/skosmos/). Skosmos is a web-based tool for browsing and publishing SKOS vocabularies, which will allow users to easily explore the created lexicons.\n",
    "\n",
    "The tutorial will be structured in the following phases:\n",
    "1. Understanding the SKOS format and its relevance for representing linguistic data.\n",
    "2. Preparing structured data for the transformation process.\n",
    "3. Implementing the transformation from structured data to SKOS.\n",
    "4. Validating, visualizing, and publishing the resulting lexicon on Skosmos.\n",
    "\n",
    "This step-by-step guide aims to provide practical insights into the use of LLOD technologies for language resource development, contributing to the wider use and accessibility of linguistic data in an open and interconnected manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972d955-6cf3-4189-bb9c-408728fb75bd",
   "metadata": {},
   "source": [
    "### **Prerequisites:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0d2c5-1198-4911-b38a-eea240a4d28a",
   "metadata": {},
   "source": [
    "To follow this tutorial successfully, you should have the following knowledge and tools:\n",
    "\n",
    "Knowledge:\n",
    "- **Linked Data**: Familiarity with the principles of Linked Data and how it enables the interconnection of data across the web.\n",
    "- **SKOS (Simple Knowledge Organization System)**: Understanding of how SKOS is used to represent structured vocabularies and taxonomies.\n",
    "- **Python**: Basic programming skills in Python, particularly for data processing and RDF generation.\n",
    "- **Skosmos**: Basic understanding of how Skosmos works as a tool for browsing and publishing SKOS-based vocabularies.\n",
    "\n",
    "Required Tools:\n",
    "- **Text Editor or IDE**: For example, Visual Studio Code, PyCharm, or any code editor you prefer for editing Python code.\n",
    "- **Jupyter Notebook**: A web-based interactive environment for Python programming, where you'll be able to run and document your code.\n",
    "- **Python** and the following libraries:\n",
    "  - `yatter`: A Python library for RDF manipulation.\n",
    "  - `morph-kgc`: A library used for transforming structured data (e.g., from relational databases or spreadsheets) into RDF.\n",
    "- **Structured Data**: You'll need your data in a structured format, either **XLSX (Excel spreadsheet)** or **JSON**, which will be transformed into SKOS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c70903-5f8a-467c-9d1a-9cd368226c07",
   "metadata": {},
   "source": [
    "### **Preparation of Vocabulary Metadata**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45c891-f28c-4e7a-b43e-3e5c602e4005",
   "metadata": {},
   "source": [
    "In this section, we will focus on retrieving the metadata of the vocabulary that has been deposited in the DSpace repository of ILC4CLARIN. This metadata is essential for documenting and structuring the vocabulary in accordance with Linked Data principles, and will serve as the foundation for defining the SKOS conceptScheme.\n",
    "\n",
    "The metadata will be extracted from the DSpace repository, which typically includes information such as authorship, date of issuance, licensing, and subject classification. We will then formalize this metadata into a schema that will describe the overarching conceptScheme for the vocabulary. This conceptScheme acts as the container for the SKOS concepts that form the core of the vocabulary, providing a structured and navigable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37de03e-ecba-4393-981c-80c98380651e",
   "metadata": {},
   "source": [
    "#### **Steps for Retrieving Metadata from DSpace Using REST API and Saving as JSON**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0c44a-1d75-4ab3-810a-65c93a1dceb3",
   "metadata": {},
   "source": [
    "**Step 1: Install and import Required Libraries**\n",
    "Before we begin, you need to install the `requests` library, which will allow you to make HTTP requests in Python.\n",
    "\n",
    "If you don't have it installed, run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8443b002-259b-4f2c-8242-80d8574b25c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: ruamel.yaml in /opt/conda/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel.yaml) (0.2.8)\n",
      "Requirement already satisfied: yatter in /opt/conda/lib/python3.11/site-packages (1.1.5)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.21 in /opt/conda/lib/python3.11/site-packages (from yatter) (0.18.0)\n",
      "Requirement already satisfied: rdflib>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from yatter) (7.0.0)\n",
      "Requirement already satisfied: coloredlogs>=15.0.1 in /opt/conda/lib/python3.11/site-packages (from yatter) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.11/site-packages (from coloredlogs>=15.0.1->yatter) (10.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.2.0->yatter) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.2.0->yatter) (3.1.4)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel.yaml>=0.17.21->yatter) (0.2.8)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.2.0->yatter) (1.16.0)\n",
      "Requirement already satisfied: morph-kgc in /opt/conda/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: duckdb<2.0.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (1.1.1)\n",
      "Requirement already satisfied: elementpath<5.0.0,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (4.5.0)\n",
      "Requirement already satisfied: falcon<4.0.0,>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (3.1.3)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (1.0.6)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (2.2.3)\n",
      "Requirement already satisfied: pyoxigraph<0.4.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (0.3.22)\n",
      "Requirement already satisfied: rdflib<8.0.0,>=6.1.1 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (7.0.0)\n",
      "Requirement already satisfied: ruamel-yaml<0.19.0,==0.18.0 in /opt/conda/lib/python3.11/site-packages (from morph-kgc) (0.18.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel-yaml<0.19.0,==0.18.0->morph-kgc) (0.2.8)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.0->morph-kgc) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.0->morph-kgc) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.0->morph-kgc) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.0->morph-kgc) (2024.2)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from rdflib<8.0.0,>=6.1.1->morph-kgc) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib<8.0.0,>=6.1.1->morph-kgc) (3.1.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib<8.0.0,>=6.1.1->morph-kgc) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install ruamel.yaml\n",
    "!pip install yatter\n",
    "!pip install morph-kgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b5dab3-cd62-4c1b-b9ae-edf60c6c150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json      \n",
    "import os        \n",
    "import hashlib   \n",
    "import urllib3   \n",
    "import warnings\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import re\n",
    "import yaml\n",
    "import yatter\n",
    "from ruamel.yaml import YAML\n",
    "import morph_kgc\n",
    "from rdflib import Graph, Namespace, URIRef\n",
    "from rdflib.namespace import SKOS, RDF  # Import RDF\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b46eff-e84a-49ae-86bd-2b168d73c00c",
   "metadata": {},
   "source": [
    "**Step 2: Search REALITER Collection Data from DSpace using REST API**\n",
    "\n",
    "In this step, we will send a GET request to retrieve data about the REALITER collection from the DSpace REST API. Specifically, we will query the collections endpoint to search for the REALITER collection within the ILC4CLARIN DSpace repository.\n",
    "\n",
    "We will use the following endpoint to retrieve the collections:\n",
    "`https://dspace-clarin-it.ilc.cnr.it/repository/rest/collections`\n",
    "\n",
    "Below is the code to perform this GET request using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0c52f8-51bf-40f6-942a-c8f282e33253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:32,810 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n",
      "2024-10-09 15:35:33,131 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/collections HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "{\n",
      "    \"type\": \"collection\",\n",
      "    \"expand\": [\n",
      "        \"parentCommunityList\",\n",
      "        \"parentCommunity\",\n",
      "        \"items\",\n",
      "        \"license\",\n",
      "        \"logo\",\n",
      "        \"all\"\n",
      "    ],\n",
      "    \"handle\": \"000-c0-111/565\",\n",
      "    \"id\": 25,\n",
      "    \"name\": \"REALITER - OTPL\",\n",
      "    \"copyrightText\": \"\",\n",
      "    \"introductoryText\": \"<p>REALITER \\u00e8 la Rete panlatina di terminologia che riunisce individui, istituzioni e organismi dei Paesi di lingua neolatina che lavorano nel settore della terminologia, al fine di favorire lo sviluppo armonizzato delle lingue neolatine.</p>\\r\\n<p>L'OTPL - Osservatorio di terminologie e politiche linguistiche sviluppa studi sulle terminologie specialistiche nelle lingue euroamericane, attraverso attivit\\u00e0 di ricerca scientifica, teorica e applicata, in prospettiva diacronica e sincronica.</p>\\r\\n<p></p>\\r\\n <p>REALITER is the Pan-Latin Terminology Network that brings together individuals, institutions and organizations from the Latin-speaking countries working in the field of terminology in order to support the harmonized development of Latin languages.</p>\\r\\n<p>OTPL - Observatory of Terminologies and Language Policies develops studies on specialized terminologies in Euro-American languages through scientific research activities, both theoretical and applied, in a diachronic and synchronic perspective.</p>\",\n",
      "    \"items\": [],\n",
      "    \"numberItems\": 20,\n",
      "    \"parentCommunityList\": [],\n",
      "    \"shortDescription\": \"REALITER - OTPL Collection\",\n",
      "    \"sidebarText\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Suppress only the InsecureRequestWarning from urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Define the API endpoint for collections\n",
    "url = \"https://dspace-clarin-it.ilc.cnr.it/repository/rest/collections\"\n",
    "\n",
    "# Make the GET request to the DSpace API, disabling SSL verification\n",
    "response = requests.get(url, verify=False)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "# Get the data from the response\n",
    "collections_data = response.json()\n",
    "\n",
    "# Filter the first collection that contains \"realiter\" in the \"name\" field\n",
    "realiter_collection = next((collection for collection in collections_data if \"realiter\" in collection['name'].lower()), None)\n",
    "\n",
    "# Print the filtered REALITER collection (optional)\n",
    "if realiter_collection:\n",
    "    print(json.dumps(realiter_collection, indent=4))\n",
    "else:\n",
    "    print(\"No REALITER collection found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce79b2-82e6-428b-beb4-56928f2e3191",
   "metadata": {},
   "source": [
    "**Step 3: Retrieve Items from the REALITER Collection**\n",
    "\n",
    "In this step, we will retrieve the items from the REALITER collection using the DSpace REST API. After identifying the REALITER collection in the previous step, we will now query the items that belong to this collection. \n",
    "\n",
    "The API endpoint to retrieve the items from a specific collection is:  \n",
    "`https://dspace-clarin-it.ilc.cnr.it/repository/rest/collections/{collectionId}/items`\n",
    "\n",
    "We will use the `collectionId` from the `realiter_collection` obtained in Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f5c9738-024a-4d27-b5fd-ce20ad5f2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:33,161 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n",
      "2024-10-09 15:35:33,358 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/collections/25/items HTTP/11\" 200 6117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items retrieved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Assuming realiter_collection has been retrieved in Step 2\n",
    "collection_id = realiter_collection['id']  # Extract the ID of the REALITER collection\n",
    "\n",
    "# Construct the API endpoint for retrieving items in the REALITER collection\n",
    "items_url = f\"https://dspace-clarin-it.ilc.cnr.it/repository/rest/collections/{collection_id}/items\"\n",
    "\n",
    "# Make the GET request to the DSpace API to retrieve the items\n",
    "response = requests.get(items_url, verify=False)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Items retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "# Get the data from the response (list of items)\n",
    "items_data = response.json()\n",
    "\n",
    "# Print the retrieved items (optional)\n",
    "#print(json.dumps(items_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc7aa0-2d64-4114-8aad-1a8fb00195c4",
   "metadata": {},
   "source": [
    "**Step 4: Get the Vocabularies Metadata**\n",
    "\n",
    "In this section, we will retrieve the metadata for each item in the REALITER collection. The metadata provides detailed information about each vocabulary item, such as authorship, title, language, and more. We will use the DSpace REST API endpoint:\n",
    "\n",
    "```\n",
    "https://dspace-clarin-it.ilc.cnr.it/repository/rest/items/{item_id}/metadata\n",
    "```\n",
    "\n",
    "For each item in our list, we will extract its `id`, then make a request to this endpoint to retrieve the associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca15042-8be7-40f2-b15b-02d24ab2b1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:33,386 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n",
      "2024-10-09 15:35:33,553 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/618/metadata HTTP/11\" 200 2835\n",
      "2024-10-09 15:35:33,563 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:33,727 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1077/metadata HTTP/11\" 200 3597\n",
      "2024-10-09 15:35:33,737 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:33,898 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1087/metadata HTTP/11\" 200 2765\n",
      "2024-10-09 15:35:33,906 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:34,069 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1091/metadata HTTP/11\" 200 2745\n",
      "2024-10-09 15:35:34,079 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:34,241 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1089/metadata HTTP/11\" 200 2734\n",
      "2024-10-09 15:35:34,251 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:34,412 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1107/metadata HTTP/11\" 200 3134\n",
      "2024-10-09 15:35:34,423 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:34,583 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1118/metadata HTTP/11\" 200 3023\n",
      "2024-10-09 15:35:34,592 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:34,756 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1130/metadata HTTP/11\" 200 2912\n",
      "2024-10-09 15:35:34,765 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:34,930 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1132/metadata HTTP/11\" 200 3025\n",
      "2024-10-09 15:35:34,940 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:35,105 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1133/metadata HTTP/11\" 200 2998\n",
      "2024-10-09 15:35:35,116 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:35,279 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1134/metadata HTTP/11\" 200 2764\n",
      "2024-10-09 15:35:35,291 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:35,455 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1135/metadata HTTP/11\" 200 3153\n",
      "2024-10-09 15:35:35,465 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:35,625 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1136/metadata HTTP/11\" 200 2812\n",
      "2024-10-09 15:35:35,633 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:35,795 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1138/metadata HTTP/11\" 200 2677\n",
      "2024-10-09 15:35:35,804 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:35,971 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1137/metadata HTTP/11\" 200 2494\n",
      "2024-10-09 15:35:35,980 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:36,143 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1139/metadata HTTP/11\" 200 2814\n",
      "2024-10-09 15:35:36,151 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:36,313 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1140/metadata HTTP/11\" 200 3296\n",
      "2024-10-09 15:35:36,322 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:36,483 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1141/metadata HTTP/11\" 200 2898\n",
      "2024-10-09 15:35:36,493 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:36,657 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1142/metadata HTTP/11\" 200 3680\n",
      "2024-10-09 15:35:36,665 | DEBUG: Starting new HTTPS connection (1): dspace-clarin-it.ilc.cnr.it:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:35:36,828 | DEBUG: https://dspace-clarin-it.ilc.cnr.it:443 \"GET /repository/rest/items/1143/metadata HTTP/11\" 200 2959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata retrieved for item 1143\n"
     ]
    }
   ],
   "source": [
    "# Assuming items_data has been populated in Step 3\n",
    "metadata_data = []\n",
    "\n",
    "# Loop through each item to get the metadata\n",
    "for item in items_data:\n",
    "    item_id = item['id']  # Extract the item ID\n",
    "    metadata_url = f\"https://dspace-clarin-it.ilc.cnr.it/repository/rest/items/{item_id}/metadata\"\n",
    "\n",
    "    # Make the GET request to retrieve the metadata for the current item\n",
    "    response = requests.get(metadata_url, verify=False)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        metadata = response.json()  # Get the metadata data\n",
    "        print(f\"Metadata retrieved for item {item_id}\")\n",
    "        metadata_data.append({\n",
    "            \"item_id\": item_id,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve metadata for item {item_id} with status code: {response.status_code}\")\n",
    "\n",
    "# Print the final metadata for all items (optional)\n",
    "# print(json.dumps(metadata_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f5300-46b5-4e73-bb7a-c815f758f630",
   "metadata": {},
   "source": [
    "**Step 5: Metadata Mapping and Saving Concept Schemes Metadata**\n",
    "\n",
    "In this step, we will map the metadata retrieved for each item in the REALITER collection into a structured format suitable for creating a SKOS Concept Scheme. Each item's metadata will be transformed into a Python dictionary, where the `key` fields from the metadata will be used as the dictionary keys, and the corresponding `value` fields will be stored as values.\n",
    "\n",
    "This will help us organize the metadata in a structured format that can be used to generate SKOS concept schemes or to integrate with other linked data frameworks.\n",
    "\n",
    "##### Metadata Structure:\n",
    "\n",
    "Given the metadata structure of each item, we will create a dictionary where:\n",
    "- The keys will be unique `key` fields like `\"dc.title\"`, `\"dc.contributor.author\"`, `\"dc.language.iso\"`, etc.\n",
    "- The values will be the corresponding `value` fields.\n",
    "- For keys that can have multiple values (e.g., `dc.contributor.author` or `dc.language.iso`), the values will be stored as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa66c80-e069-4fa2-8450-884ccf64f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: concept_schemes/20_500_11752_OPEN_975.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_987.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_993.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_994.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_995.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1006.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1008.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1014.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1015.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1016.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1017.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1018.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1019.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1021.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1020.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1022.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1024.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1025.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1026.json\n",
      "File saved: concept_schemes/20_500_11752_OPEN_1027.json\n",
      "{\n",
      "    \"item_id\": 618,\n",
      "    \"metadata\": [\n",
      "        {\n",
      "            \"key\": \"dc.contributor.author\",\n",
      "            \"value\": \"Dankova, Klara\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.contributor.author\",\n",
      "            \"value\": \"Zanola, Maria Teresa\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.contributor.author\",\n",
      "            \"value\": \"Calvi, Silvia\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.date.accessioned\",\n",
      "            \"value\": \"2022-02-02T15:07:20Z\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.date.available\",\n",
      "            \"value\": \"2022-02-02T15:07:20Z\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.date.issued\",\n",
      "            \"value\": \"2022-01-21\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.identifier.uri\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"http://hdl.handle.net/20.500.11752/OPEN-975\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.description\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"The Pan-Latin Textile Fibres Vocabulary (Lessico panlatino delle fibre tessili), developed within the Realiter network, contains the basic terms designating textile fibres in seven Romance languages (Italian, Catalan, Spanish, French, Galician, Portuguese, Romanian) and in English.\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"ita\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"cat\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"spa\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"fra\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"glg\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"por\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"ron\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.language.iso\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"eng\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.publisher\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"Educatt\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.rights\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"Creative Commons - Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.rights.uri\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"http://creativecommons.org/licenses/by-sa/4.0/\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.rights.label\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"PUB\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.source.uri\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"https://www.realiter.net/\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.subject\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"textile fibres\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.subject\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"Terminology\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.subject\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"Romance languages\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.title\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"Pan-Latin Textile Fibres Vocabulary\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"dc.type\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"lexicalConceptualResource\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"metashare.ResourceInfo#ContentInfo.detailedType\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"terminilogicalResource\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"metashare.ResourceInfo#ContentInfo.mediaType\",\n",
      "            \"language\": \"en_US\",\n",
      "            \"value\": \"text\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.has.files\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"yes\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.branding\",\n",
      "            \"value\": \"OPEN\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.contact.person\",\n",
      "            \"value\": \"Klara@@Dankova@@klara.dankova@unicatt.it@@Universit\\u00e0 Cattolica del Sacro Cuore\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.contact.person\",\n",
      "            \"value\": \"Maria Teresa@@Zanola@@realiter@unicatt.it@@Realiter\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.contact.person\",\n",
      "            \"value\": \"Maria Teresa@@Zanola@@osservatorio.terminologie@unicatt.it@@Osservatorio di terminologie e politiche linguistiche\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.size.info\",\n",
      "            \"value\": \"70@@entries\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.files.size\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"27908\"\n",
      "        },\n",
      "        {\n",
      "            \"key\": \"local.files.count\",\n",
      "            \"language\": \"*\",\n",
      "            \"value\": \"1\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mapped_metadata_list = []\n",
    "\n",
    "# Loop through the metadata of each item in metadata_data\n",
    "for item in metadata_data:\n",
    "    item_id = item['item_id']  # Extract item_id\n",
    "    metadata = item['metadata']  # Get the metadata list\n",
    "    \n",
    "    # Create a dictionary to store the mapped metadata\n",
    "    mapped_metadata = {\"item_id\": item_id}\n",
    "\n",
    "    # Loop through each metadata element\n",
    "    for element in metadata:\n",
    "        key = element['key'].replace('.', '_')  # Replace '.' with '_'\n",
    "        value = element['value']\n",
    "\n",
    "        # Special case for 'dc_language_iso' to convert it to a list of objects\n",
    "        if key == 'dc_language_iso':\n",
    "            if key in mapped_metadata:\n",
    "                if isinstance(mapped_metadata[key], list):\n",
    "                    # Add the new value as an object with a \"code\" key\n",
    "                    mapped_metadata[key].append({\"code\": value})\n",
    "                else:\n",
    "                    # Convert the single value to a list of objects with \"code\" keys\n",
    "                    mapped_metadata[key] = [{\"code\": mapped_metadata[key]}, {\"code\": value}]\n",
    "            else:\n",
    "                # If the key doesn't exist, create a list of objects with \"code\" keys\n",
    "                mapped_metadata[key] = [{\"code\": value}]\n",
    "        else:\n",
    "            # If the key already exists, append the value to the list (to handle multiple values for the same key)\n",
    "            if key in mapped_metadata:\n",
    "                if isinstance(mapped_metadata[key], list):\n",
    "                    mapped_metadata[key].append(value)\n",
    "                else:\n",
    "                    # Convert the single value to a list and append the new value\n",
    "                    mapped_metadata[key] = [mapped_metadata[key], value]\n",
    "            else:\n",
    "                # If the key doesn't exist, add it to the dictionary\n",
    "                mapped_metadata[key] = value\n",
    "\n",
    "    # Append the mapped metadata dictionary to the final list\n",
    "    mapped_metadata_list.append(mapped_metadata)\n",
    "\n",
    "# Create the directory \"concept_schemes\" if it doesn't exist\n",
    "output_directory = \"concept_schemes\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Loop through each mapped metadata and save as JSON file\n",
    "for mapped_metadata in mapped_metadata_list:\n",
    "    # Extract the value of \"dc_identifier_uri\"\n",
    "    if \"dc_identifier_uri\" in mapped_metadata:\n",
    "        identifier_uri = mapped_metadata[\"dc_identifier_uri\"]\n",
    "        \n",
    "        # Extract the part after \"http://hdl.handle.net/\"\n",
    "        if identifier_uri.startswith(\"http://hdl.handle.net/\"):\n",
    "            identifier_suffix = identifier_uri.replace(\"http://hdl.handle.net/\", \"\").replace(\"/\", \"_\")\n",
    "            identifier_suffix = identifier_suffix.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "            # Define the file name and path\n",
    "            file_name = f\"{identifier_suffix}.json\"\n",
    "            file_path = os.path.join(output_directory, file_name)\n",
    "            \n",
    "            # Save the mapped metadata as a JSON file\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(mapped_metadata, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(f\"File saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Missing 'dc.identifier.uri' for item {mapped_metadata['item_id']}\")\n",
    "\n",
    "# Optionally print out the first item in metadata_data for verification\n",
    "print(json.dumps(metadata_data[0], indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868545ad-2f1a-4736-aecc-6051fd0756e4",
   "metadata": {},
   "source": [
    "### **Preparation of Source Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec29251-153a-4613-af1f-d4dbb2052ab6",
   "metadata": {},
   "source": [
    "In this section, we will explain how to prepare the structured files so they can be properly associated with their corresponding metadata. \n",
    "\n",
    "\n",
    "**Step 1: Renaming input files**\n",
    "\n",
    "To ensure a correct association between structured data files and their respective metadata files (Concept Schemes), the structured files must be renamed using the same nomenclature as the corresponding Concept Scheme files. This is a manual process and requires the following steps:\n",
    "\n",
    "1. **Identify the Corresponding Metadata**: \n",
    "   For each structured file (e.g., a lexicon or terminology file), you need to identify its matching Concept Scheme file. This can be done by checking the metadata that describes the structured file. Usually, this metadata is stored in a JSON file and contains information such as the `dc.identifier.uri` which uniquely identifies the Concept Scheme.\n",
    "\n",
    "2. **Renaming the Structured Files**: \n",
    "   Once you have identified the corresponding Concept Scheme, rename the structured file to match the file name of the Concept Scheme. For instance, if the Concept Scheme file is named `20_500_11752_OPEN_1014.json`, you should rename the structured file to `20_500_11752_OPEN_1014.<appropriate_extension>` (e.g., `.xlsx`, `.json`, etc.). This ensures that both the structured file and its metadata share the same base name.\n",
    "\n",
    "3. **Consistency Check**: \n",
    "   After renaming the files, verify that the filenames are consistent across all related files (metadata, structured data). This step is crucial to ensure proper mapping during the processing phase. Misnamed files could cause errors or incorrect associations between data and metadata.\n",
    "\n",
    "This manual process is essential for preparing the data to ensure it can be processed correctly in subsequent stages of the pipeline. Properly named files will ensure that structured data can be automatically linked to the correct metadata during mapping and transformation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7f0e3-890d-4c42-8243-5d942288f1f8",
   "metadata": {},
   "source": [
    "**Step 2: adapt and normalize source files**\n",
    "\n",
    "The main objective of this step is to create a new data structure that makes the information more accessible and aligned with the requirements for Linked Data. By following a lexicon-building syntax in Excel, we will extract key elements such as preferred labels (prefLabel), alternative labels (altLabel), and any notes or additional information associated with each term.\n",
    "\n",
    " This process will allow us to create intermediate JSON files that contain normalized data. These JSON files will be saved in the same folder as the structured source files. The goal is to transform and standardize the input data into a consistent format, making it easier to convert into Linked Data and prepare it for subsequent processing steps.\n",
    "\n",
    "By adapting and normalizing the source files, we ensure that all values, labels, and language tags are uniform across the dataset. This includes handling language variations, cleaning and transforming labels, and organizing the data in a structured format that will be used for further transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0353eb-ace8-49da-8350-1ee7f804fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 20_500_11752_OPEN_1015.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1015.json\n",
      "Processing file: 20_500_11752_OPEN_1017.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1017.json\n",
      "Processing file: 20_500_11752_OPEN_1021.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1021.json\n",
      "Processing file: 20_500_11752_OPEN_1022.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1022.json\n",
      "Processing file: 20_500_11752_OPEN_1020.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1020.json\n",
      "Processing file: 20_500_11752_OPEN_1024.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1024.json\n",
      "Processing file: 20_500_11752_OPEN_1016.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1016.json\n",
      "Processing file: 20_500_11752_OPEN_1019.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1019.json\n",
      "Processing file: 20_500_11752_OPEN_1026.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1026.json\n",
      "Processing file: 20_500_11752_OPEN_1018.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1018.json\n",
      "Processing file: 20_500_11752_OPEN_1014.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1014.json\n",
      "Processing file: 20_500_11752_OPEN_1027.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1027.json\n",
      "Processing file: 20_500_11752_OPEN_1025.xlsx\n",
      "Processed data saved to input_data/20_500_11752_OPEN_1025.json\n"
     ]
    }
   ],
   "source": [
    "def clean_column_name(col_name):\n",
    "    try:\n",
    "        # Converti in minuscolo e rimuovi spazi esterni\n",
    "        col_name = col_name.lower().strip()\n",
    "        \n",
    "        # Sostituzione parentesi quadre con trattini e rimozione degli spazi attorno al trattino\n",
    "        if '[' in col_name and ']' in col_name:\n",
    "            col_name = col_name.replace('[', '-').replace(']', '')\n",
    "            col_name = col_name.replace(\" -\", \"-\").replace(\"- \", \"-\")\n",
    "        \n",
    "        # Mappatura delle combinazioni di lingue\n",
    "        lang_map = {\n",
    "            \"es-es\": \"es\",\n",
    "            \"es-mex\": \"es-MX\",\n",
    "            \"es-arg\": \"es-AR\",\n",
    "            \"pt-br\": \"pt-BR\",\n",
    "            \"fr-ca\": \"fr-CA\",\n",
    "        }\n",
    "\n",
    "        # Gestione dei casi con lingue multiple, come \"es-arg/mex\"\n",
    "        if \"/\" in col_name:\n",
    "            # Dividi le lingue e trova la prima valida nella mappa\n",
    "            parts = col_name.split(\"/\")\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part in lang_map:\n",
    "                    return lang_map[part]\n",
    "            # Se nessuna parte è nella mappa, ritorna la prima parte\n",
    "            return parts[0]\n",
    "\n",
    "        # Verifica se col_name corrisponde a una chiave nella mappa e sostituiscilo\n",
    "        if col_name in lang_map:\n",
    "            col_name = lang_map[col_name]\n",
    "\n",
    "        return col_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning column name: {col_name}. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def process_value(value):\n",
    "    try:\n",
    "        # Verifica se il valore è una Serie (potrebbe essere accaduto accidentalmente)\n",
    "        if isinstance(value, pd.Series):\n",
    "            # Se è una Serie, prendi il primo elemento (o qualsiasi altra logica che abbia senso)\n",
    "            value = value.iloc[0]\n",
    "        \n",
    "        # Controlla se il valore è NaN o vuoto\n",
    "        if pd.isna(value) or value == \"\":\n",
    "            return [], []\n",
    "        \n",
    "        # Gestione dei numeri\n",
    "        if isinstance(value, (int, float)):  # Handle numerical values\n",
    "            value = str(value).strip()\n",
    "            return value, []\n",
    "        \n",
    "        # Gestione delle stringhe\n",
    "        if isinstance(value, str):\n",
    "            # Dividi la stringa in base alle nuove righe (se presenti)\n",
    "            values = [v.strip() for v in value.split(\"\\n\") if v.strip()]\n",
    "            \n",
    "            # Prima etichetta è prefLabel, le altre sono altLabels\n",
    "            pref_label = values[0] if values else None\n",
    "            alt_labels = values[1:] if len(values) > 1 else []\n",
    "            return pref_label, alt_labels\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unsupported value type: {type(value)} with value: {value}\")\n",
    "            return [], []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing value: {value}. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def clean_concept_name(concept):\n",
    "    try:\n",
    "        # Split the concept based on \\n and take the first value\n",
    "        first_part = concept.split(\"\\n\")[0].strip()\n",
    "\n",
    "        # Use a regex to remove content inside the last parentheses, if present\n",
    "        cleaned_concept = re.sub(r'\\s*\\([^)]+\\)\\s*$', '', first_part).strip()\n",
    "\n",
    "        # Replace problematic characters: replace spaces and dashes with underscores\n",
    "        cleaned_concept = cleaned_concept.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"|\", \"_\").replace(\"'\", \"_\").replace(\",\", \"_\").replace(\"’\", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "        # Check if the string contains a \"(\" character\n",
    "        if \"(\" in cleaned_concept:\n",
    "            print(f\"Warning: Concept contains '(': {cleaned_concept}\")\n",
    "\n",
    "        \n",
    "        # Return the cleaned concept without any encoding\n",
    "        return cleaned_concept\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing concept: {concept}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_definition(row, first_col):\n",
    "    try:\n",
    "        for col in row.index:\n",
    "            if col.lower() == 'def':\n",
    "                definition = row[col]\n",
    "                definition_lang = clean_column_name(first_col)  # Utilizza il nome pulito della prima colonna come lingua\n",
    "                return definition, definition_lang\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting definition from row: {row}. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def extract_value(label):\n",
    "    # Pattern per rimuovere il contenuto tra le ultime parentesi tonde\n",
    "    cleaned_value = re.sub(r'\\s*\\([^)]+\\)\\s*$', '', label).strip()\n",
    "    return cleaned_value\n",
    "\n",
    "\n",
    "def process_excel_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Pulizia dei nomi delle colonne e controllo di compatibilità\n",
    "        original_columns = df.columns.tolist()  # Conserva le colonne originali\n",
    "        cleaned_columns = [clean_column_name(col) for col in original_columns]\n",
    "\n",
    "        if len(cleaned_columns) != len(original_columns):\n",
    "            print(f\"Warning: The number of cleaned columns ({len(cleaned_columns)}) does not match the number of original columns ({len(original_columns)}).\")\n",
    "\n",
    "        df.columns = cleaned_columns  # Applica i nomi delle colonne puliti\n",
    "        \n",
    "        processed_data = []\n",
    "        for index, row in df.iterrows():\n",
    "            # Sostituisci i NaN con stringhe vuote nella riga\n",
    "            row = row.fillna(\"\")\n",
    "\n",
    "            concept_raw = row.iloc[0]\n",
    "            concept = clean_concept_name(concept_raw)\n",
    "\n",
    "            # Recupera la definizione e la lingua\n",
    "            first_col = df.columns[0]  # Ottieni il nome della prima colonna\n",
    "            definition, definition_lang = get_definition(row, first_col)\n",
    "\n",
    "            pref_labels = []\n",
    "            alt_labels = []\n",
    "            note = None \n",
    "\n",
    "            # Controlla se esiste una colonna \"nota\" indipendentemente dal caso\n",
    "            note_column = next((col for col in df.columns if col.lower() in ['nota']), None)\n",
    "            notes = []  # Lista per contenere tutte le note\n",
    "            \n",
    "            if note_column:\n",
    "                raw_note = row.get(note_column, \"\")\n",
    "                \n",
    "                # Verifica se ci sono numerazioni (es. \"1.\", \"2.\", ecc.)\n",
    "                numbered_notes = re.split(r'\\s*\\d+\\.\\s*', raw_note.strip())\n",
    "            \n",
    "                # Se ci sono più elementi dopo lo split, allora ci sono numerazioni\n",
    "                if len(numbered_notes) > 1:\n",
    "                    for note_text in numbered_notes:\n",
    "                        if note_text.strip():\n",
    "                            notes.append({\n",
    "                                \"concept\": concept,\n",
    "                                \"value\": note_text.strip(),\n",
    "                                \"lang\": None\n",
    "                            })\n",
    "                else:\n",
    "                    notes.append({\n",
    "                        \"concept\": concept,\n",
    "                        \"value\": raw_note.strip(),\n",
    "                        \"lang\": None  \n",
    "                    })\n",
    "\n",
    "            for col in df.columns:\n",
    "                if col.lower() == 'def' or col == 'nota':\n",
    "                    continue\n",
    "                \n",
    "                cell_value = row[col]\n",
    "                pref_label, alt_label_list = process_value(cell_value)\n",
    "\n",
    "                # Per la prefLabel\n",
    "                if pref_label:\n",
    "                    pref_labels.append({\n",
    "                        \"concept\": concept,\n",
    "                        \"value\": extract_value(pref_label),\n",
    "                        \"lang\": col,\n",
    "                    })\n",
    "                    notes.append({\n",
    "                        \"concept\": concept,\n",
    "                        \"value\": pref_label.strip(),\n",
    "                        \"lang\": col\n",
    "                    })\n",
    "                \n",
    "                # Per le altLabel\n",
    "                for alt_label in alt_label_list:\n",
    "                    alt_labels.append({\n",
    "                        \"concept\": concept,\n",
    "                        \"value\": extract_value(alt_label),\n",
    "                        \"lang\": col,\n",
    "                    })\n",
    "\n",
    "            note = notes if notes else None\n",
    "\n",
    "            json_structure = {\n",
    "                \"concept\": concept,\n",
    "                \"definition\": definition,\n",
    "                \"definitionLang\": definition_lang,  # Aggiungi il campo della lingua della definizione\n",
    "                \"prefLabels\": pref_labels,\n",
    "                \"altLabels\": alt_labels,\n",
    "                \"note\": note\n",
    "            }\n",
    "            \n",
    "            processed_data.append(json_structure)\n",
    "        \n",
    "        return processed_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {file_path}. Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_all_excel_files(input_data_dir):\n",
    "    all_processed_data = {}\n",
    "    \n",
    "    for filename in os.listdir(input_data_dir):\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(input_data_dir, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                processed_data = process_excel_file(file_path)\n",
    "                if processed_data:\n",
    "                    all_processed_data[filename] = processed_data\n",
    "                \n",
    "                    output_filename = filename.replace(\".xlsx\", \".json\")\n",
    "                    output_path = os.path.join(input_data_dir, output_filename)\n",
    "                    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(processed_data, f, ensure_ascii=False, indent=4)\n",
    "                    \n",
    "                    print(f\"Processed data saved to {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    return all_processed_data\n",
    "\n",
    "# Directory containing input Excel files\n",
    "input_data_dir = \"input_data\"\n",
    "\n",
    "# Process all Excel files in the directory\n",
    "processed_data = process_all_excel_files(input_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86ae1c-2b21-4f61-bbce-e5c05f289f41",
   "metadata": {},
   "source": [
    "### **Creation of YARRRML Mapping File**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0e66e-e8f1-4c43-ac02-df6cfa8ce029",
   "metadata": {},
   "source": [
    "After obtaining the vocabulary metadata and structured data, the next step is to create a YARRRML mapping file for each vocabulary. This mapping file will allow us to define how the data from our structured sources is transformed into RDF triples, which are essential for generating SKOS Concept Schemes.\n",
    "\n",
    "YARRRML (Yet Another RML Mapping Language) is a user-friendly, YAML-based syntax for creating RML (RDF Mapping Language) mappings. It simplifies the process of describing how data should be mapped to RDF by allowing the use of a more readable format.\n",
    "\n",
    "#### **Purpose of the Mapping File**\n",
    "\n",
    "The YARRRML mapping file serves as the blueprint that specifies how the structured data (e.g., terms, definitions, languages, and relationships) will be transformed into RDF triples in SKOS format. Each mapping file will correspond to a specific vocabulary and will contain rules for converting the data into SKOS concepts, labels, definitions, and relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c9112-dbe9-4830-9314-31c553098f07",
   "metadata": {},
   "source": [
    "**Step 1: Collecting references**\n",
    "\n",
    "In this step, we will iterate over the `bitstreams_data_no_duplicates` variable to collect references that link each vocabulary's concept schema and its corresponding structured data file. The goal is to match the `item_id` from the `bitstreams_data_no_duplicates` to the appropriate JSON file inside the `concept_schemes` directory, as well as retrieve the corresponding structured data file from the `input_data` directory based on the `name` attribute found within the bitstreams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8944c51d-e544-48fe-9281-bc6e6fd9ef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data pairs collected: 20\n",
      "[{'references': '20_500_11752_OPEN_1021.json'}, {'references': '20_500_11752_OPEN_1017.json'}, {'references': '20_500_11752_OPEN_1014.json'}, {'references': '20_500_11752_OPEN_1022.json'}, {'references': '20_500_11752_OPEN_1008.json'}, {'references': '20_500_11752_OPEN_1015.json'}, {'references': '20_500_11752_OPEN_1019.json'}, {'references': '20_500_11752_OPEN_1006.json'}, {'references': '20_500_11752_OPEN_1025.json'}, {'references': '20_500_11752_OPEN_1016.json'}, {'references': '20_500_11752_OPEN_1024.json'}, {'references': '20_500_11752_OPEN_994.json'}, {'references': '20_500_11752_OPEN_995.json'}, {'references': '20_500_11752_OPEN_1026.json'}, {'references': '20_500_11752_OPEN_1027.json'}, {'references': '20_500_11752_OPEN_975.json'}, {'references': '20_500_11752_OPEN_1020.json'}, {'references': '20_500_11752_OPEN_1018.json'}, {'references': '20_500_11752_OPEN_987.json'}, {'references': '20_500_11752_OPEN_993.json'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Directory containing the concept schema files\n",
    "concept_schemes_dir = \"concept_schemes\"\n",
    "\n",
    "# Lista per raccogliere tutte le coppie di concept schema e structured data\n",
    "data_pairs = []\n",
    "\n",
    "# Iterate through the files in the \"concept_schemes\" directory\n",
    "for filename in os.listdir(concept_schemes_dir):\n",
    "    # Ensure the file is a JSON file\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(concept_schemes_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                file_data = json.load(f)\n",
    "                # Append the concept schema reference to the data_pairs list\n",
    "                data_pairs.append({\n",
    "                    \"references\": filename  # Store the filename as the reference\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to decode JSON from {file_path}\")\n",
    "\n",
    "# Print or process the final data_pairs list\n",
    "print(f\"Total data pairs collected: {len(data_pairs)}\")\n",
    "print(data_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b6d6b-5b63-4c77-9e7d-891733891bc2",
   "metadata": {},
   "source": [
    "**Step 2: Creation of YARRRML Mapping Files**\n",
    "\n",
    "In this step, we will generate the YARRRML mapping files, which will be used to map the data from the structured files into RDF format. After collecting the references to the pairs of files related to the conceptSchema and the structured data in the previous step, we now proceed with the creation of a directory called mapping_files. This directory will contain a separate YARRRML mapping file for each object present in the data_pairs list.\n",
    "\n",
    "The process involves iterating over the data_pairs list. For each object in the list, a YAML file will be generated in the mapping_files directory. Each YAML file will define the rules for mapping the structured data from the source file to the corresponding RDF triples according to the concept schema. These mappings will ensure that each term, label, and relationship in the source data is correctly transformed into the appropriate linked data representation.\n",
    "\n",
    "The generation of these mapping files is crucial for the next steps, where we will use these YARRRML files to create RML files that can be processed to produce the final Turtle RDF files. Each mapping file will be customized according to the structure of the concept schema and the data within each structured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989617c9-32c6-40ac-a159-fe69b2a71781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data pair 1/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1021.yaml\n",
      "Processing data pair 2/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1017.yaml\n",
      "Processing data pair 3/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1014.yaml\n",
      "Processing data pair 4/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1022.yaml\n",
      "Processing data pair 5/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1008.yaml\n",
      "Processing data pair 6/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1015.yaml\n",
      "Processing data pair 7/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1019.yaml\n",
      "Processing data pair 8/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1006.yaml\n",
      "Processing data pair 9/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1025.yaml\n",
      "Processing data pair 10/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1016.yaml\n",
      "Processing data pair 11/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1024.yaml\n",
      "Processing data pair 12/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_994.yaml\n",
      "Processing data pair 13/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_995.yaml\n",
      "Processing data pair 14/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1026.yaml\n",
      "Processing data pair 15/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1027.yaml\n",
      "Processing data pair 16/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_975.yaml\n",
      "Processing data pair 17/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1020.yaml\n",
      "Processing data pair 18/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_1018.yaml\n",
      "Processing data pair 19/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_987.yaml\n",
      "Processing data pair 20/20\n",
      "YARRRML mapping file created at: mapping_files/pl_20_500_11752_OPEN_993.yaml\n"
     ]
    }
   ],
   "source": [
    "def ensure_directory_exists(directory):\n",
    "    \"\"\"Ensure the directory exists, otherwise create it.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load a JSON file from a given path.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def extract_uri_info(dc_identifier_uri):\n",
    "    \"\"\"Extract the central value and suffix from the dc.identifier.uri.\"\"\"\n",
    "    uri_match = re.search(r'(\\d+\\.\\d+\\.\\d+)/(.+)', dc_identifier_uri)\n",
    "    if uri_match:\n",
    "        uri_number = uri_match.group(1)\n",
    "        uri_suffix = uri_match.group(2)\n",
    "        return uri_number, uri_suffix\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def build_custom_name_and_prefix(uri_number, uri_suffix, structured_file_ref):\n",
    "    \"\"\"Build the custom name and prefix based on URI information.\"\"\"\n",
    "    if uri_number and uri_suffix:\n",
    "        custom_name = f\"pl_{uri_number.replace('.', '-')}-{uri_suffix}\"\n",
    "        custom_prefix = f\"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{custom_name}/\"\n",
    "    else:\n",
    "        custom_name = f\"pl_{structured_file_ref.replace('.json', '').replace(' ', '_')}\"\n",
    "        custom_prefix = f\"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{custom_name}/\"\n",
    "    return custom_name, custom_prefix\n",
    "\n",
    "\n",
    "def create_mappings_section(custom_name, concept_schema_file, concept_schemes_dir, custom_prefix):\n",
    "    \"\"\"Create the mappings section for the concept schema as raw text.\"\"\"\n",
    "    \n",
    "    concept_schema_path = os.path.abspath(os.path.join(concept_schemes_dir, concept_schema_file))\n",
    "    \n",
    "    raw_mapping = f\"\"\"\n",
    "    {custom_name}_concept_scheme:\n",
    "        sources:\n",
    "            - [{concept_schema_path}~jsonpath, \"$\"]\n",
    "        subject: \"{custom_prefix}\"\n",
    "        predicateobjects:\n",
    "            - [a, skos:ConceptScheme]\n",
    "            - [dc:description, $(dc_description), en~lang]\n",
    "            - [dc:identifier,  $(dc_identifier_uri)]\n",
    "            - [dc:title, $(dc_title), en~lang]\n",
    "            - [dc:created, $(dc_date_issued), xsd:date]\n",
    "            - [dc:creator, $(dc_contributor_author)]\n",
    "\n",
    "\n",
    "    {custom_name}_concept_scheme_languages:\n",
    "        sources:\n",
    "            - [{concept_schema_path}~jsonpath, \"$.dc_language_iso[*]\"]\n",
    "        subject: \"{custom_prefix}\"\n",
    "        predicateobjects:\n",
    "            - [dct:language, \"http://iso639-3.sil.org/code/$(code)\"]\n",
    "\"\"\"\n",
    "    \n",
    "    return raw_mapping.strip()\n",
    "\n",
    "\n",
    "\n",
    "def create_structured_mappings_section(structured_file_ref, structured_data_file, input_data_dir):\n",
    "    \"\"\"Create the mappings section for the structured data as raw text.\"\"\"\n",
    "    \n",
    "    structured_data_path = os.path.abspath(os.path.join(input_data_dir))\n",
    "    \n",
    "    raw_mapping = f\"\"\"\n",
    "    {structured_file_ref}_concepts:\n",
    "        sources:\n",
    "            - [{structured_data_path}~jsonpath, \"$[*]\"]\n",
    "        subject: \"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{structured_file_ref}/$(concept)\"\n",
    "        predicateobjects:\n",
    "            - [a, skos:Concept]\n",
    "            - [skos:definition, $(definition), $(definitionLang)~lang]\n",
    "    \n",
    "    {structured_file_ref}_prefLabels:\n",
    "        sources:\n",
    "            - [{structured_data_path}~jsonpath, \"$[*].prefLabels[*]\"]\n",
    "        subject: \"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{structured_file_ref}/$(concept)\"\n",
    "        predicateobjects:\n",
    "            - [skos:prefLabel, $(value), $(lang)~lang]\n",
    "    \n",
    "    {structured_file_ref}_altLabels:\n",
    "        sources:\n",
    "            - [{structured_data_path}~jsonpath, \"$[*].altLabels[*]\"]\n",
    "        subject: \"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{structured_file_ref}/$(concept)\"\n",
    "        predicateobjects:\n",
    "            - [skos:altLabel, $(value), $(lang)~lang]\n",
    "    \n",
    "    {structured_file_ref}_notes:\n",
    "        sources:\n",
    "            - [{structured_data_path}~jsonpath, \"$[*].note[*]\"]\n",
    "        subject: \"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{structured_file_ref}/$(concept)\"\n",
    "        predicateobjects:\n",
    "            - [skos:note, $(value), $(lang)~lang]\n",
    "\"\"\"\n",
    "    \n",
    "    return raw_mapping.strip()\n",
    "\n",
    "\n",
    "def process_data_pairs(data_pairs, concept_schemes_dir, input_data_dir, mapping_files_dir):\n",
    "    \"\"\"Main function to iterate over data_pairs and create YARRRML files.\"\"\"\n",
    "    yaml = YAML()\n",
    "    yaml.default_flow_style = False\n",
    "    yaml.allow_unicode = True\n",
    "\n",
    "    # Prefissi di base in stile YARRRML\n",
    "    base_prefixes = {\n",
    "        \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "        \"dct\": \"http://purl.org/dc/terms/\",\n",
    "        \"iso369-3\": \"http://iso639-3.sil.org/code/\",\n",
    "        \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n",
    "        \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\n",
    "    }\n",
    "\n",
    "    for idx, data_pair in enumerate(data_pairs):\n",
    "        print(f\"Processing data pair {idx + 1}/{len(data_pairs)}\")\n",
    "\n",
    "        concept_schema_file = data_pair[\"references\"]\n",
    "        structured_file_ref = data_pair[\"references\"]\n",
    "\n",
    "        # Load concept schema JSON\n",
    "        concept_schema_path = os.path.join(concept_schemes_dir, concept_schema_file)\n",
    "        structured_path = os.path.join(input_data_dir, structured_file_ref)\n",
    "        if not os.path.exists(concept_schema_path):\n",
    "            print(f\"Concept schema file not found: {concept_schema_path}\")\n",
    "            continue\n",
    "        concept_schema = load_json(concept_schema_path)\n",
    "\n",
    "        # Extract URI and build names\n",
    "        dc_identifier_uri = concept_schema.get(\"dc.identifier.uri\", \"\")\n",
    "        #print(f\"Processing URI: {dc_identifier_uri}\")\n",
    "        uri_number, uri_suffix = extract_uri_info(dc_identifier_uri)\n",
    "        custom_name, custom_prefix = build_custom_name_and_prefix(uri_number, uri_suffix, structured_file_ref)\n",
    "\n",
    "        # Create prefixes and mappings sections\n",
    "        prefixes = base_prefixes.copy()\n",
    "        prefixes[custom_name] = custom_prefix\n",
    "\n",
    "        \n",
    "        concept_mappings_section = create_mappings_section(custom_name, concept_schema_file, concept_schemes_dir, custom_prefix)\n",
    "        structured_mappings_section = create_structured_mappings_section(custom_name, structured_file_ref, structured_path)\n",
    "\n",
    "        # Combine raw mappings sections and convert to YAML\n",
    "        raw_yarrrml_content = f\"\"\"prefixes:\n",
    "  dc: http://purl.org/dc/elements/1.1/\n",
    "  dct: http://purl.org/dc/terms/\n",
    "  iso369-3: http://iso639-3.sil.org/code/\n",
    "  skos: http://www.w3.org/2004/02/skos/core#\n",
    "  xsd: http://www.w3.org/2001/XMLSchema#\n",
    "  {custom_name}: {custom_prefix}\n",
    "\n",
    "mappings:\n",
    "    {concept_mappings_section}\n",
    "    \n",
    "    {structured_mappings_section}\n",
    "\"\"\"\n",
    "        # Write raw YAML content to file\n",
    "        yaml_filename = f\"{custom_name}.yaml\"\n",
    "        yaml_file_path = os.path.join(mapping_files_dir, yaml_filename)\n",
    "\n",
    "        with open(yaml_file_path, \"w\", encoding=\"utf-8\") as yaml_file:\n",
    "            yaml_file.write(raw_yarrrml_content)\n",
    "\n",
    "        print(f\"YARRRML mapping file created at: {yaml_file_path}\")\n",
    "        \n",
    "# Esecuzione principale\n",
    "ensure_directory_exists(\"mapping_files\")\n",
    "process_data_pairs(data_pairs, \"concept_schemes\", \"input_data\", \"mapping_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f4b48-275a-4abd-8832-b0eed484c8f5",
   "metadata": {},
   "source": [
    "### **Creation of RML Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f8b2f-d13f-4ff3-821b-22fdd289c03a",
   "metadata": {},
   "source": [
    "In this section, we will generate intermediate RML (RDF Mapping Language) files using the YARRRML-to-RML conversion library called **Yatter**. These files will serve as a bridge between our YARRRML mapping files and the final RDF output, converting the YARRRML mappings into a format that can be processed to produce RDF data.\n",
    "\n",
    "Yatter converts YARRRML YAML files into RML format, which is then used to transform structured data (e.g., CSV, JSON, Excel) into RDF triples. These intermediate RML files will be stored in the **rml_files** directory, where each file will be named according to its corresponding YARRRML file but with an `.rml` extension and serialized in Turtle format (`rdf/turtle`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31fcd3ba-a296-4463-aab8-a38b28e1e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:38,601 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,604 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,616 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,618 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,644 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,647 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,658 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,660 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,673 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,675 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,684 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,686 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,699 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,701 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,711 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,713 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,727 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,729 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,739 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,741 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,761 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,764 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,788 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,790 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,813 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,818 | INFO: RML content is created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RML file created: rml_files/pl_20_500_11752_OPEN_993.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1019.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1020.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1018.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1006.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1025.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:38,835 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,837 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,855 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,857 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,866 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,867 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,881 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,883 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,893 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,894 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,907 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,909 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,918 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,919 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,932 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,936 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,946 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,948 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,962 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,964 | INFO: RML content is created!\n",
      "2024-10-09 15:43:38,973 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:38,975 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:38,993 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:38,996 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,020 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,021 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,040 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,042 | INFO: RML content is created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RML file created: rml_files/pl_20_500_11752_OPEN_1024.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1014.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1021.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_995.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1027.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_975.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1008.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:39,051 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,052 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,063 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,065 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,075 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,077 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,089 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,091 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,100 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,102 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,115 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,117 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,127 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,129 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,142 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,144 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,155 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,156 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,171 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,173 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,183 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,184 | INFO: Translation has finished successfully.\n",
      "2024-10-09 15:43:39,197 | INFO: Translating YARRRML mapping to [R2]RML\n",
      "2024-10-09 15:43:39,199 | INFO: RML content is created!\n",
      "2024-10-09 15:43:39,209 | INFO: Mapping has been syntactically validated.\n",
      "2024-10-09 15:43:39,210 | INFO: Translation has finished successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RML file created: rml_files/pl_20_500_11752_OPEN_1016.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_987.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_994.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1017.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1022.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1015.rml\n",
      "RML file created: rml_files/pl_20_500_11752_OPEN_1026.rml\n"
     ]
    }
   ],
   "source": [
    "# Create the folder \"rml_files\" if it doesn't exist\n",
    "rml_files_dir = \"rml_files\"\n",
    "if not os.path.exists(rml_files_dir):\n",
    "    os.makedirs(rml_files_dir)\n",
    "\n",
    "# Directory containing the YARRRML mapping files\n",
    "mapping_files_dir = \"mapping_files\"\n",
    "\n",
    "yaml = YAML(typ='safe', pure=True)\n",
    "\n",
    "# Loop through each YARRRML file in the mapping_files directory\n",
    "for filename in os.listdir(mapping_files_dir):\n",
    "    if filename.endswith(\".yaml\"):  # Only process YARRRML files\n",
    "        yarrrml_file_path = os.path.join(mapping_files_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Load the YARRRML file and convert to RML\n",
    "            with open(yarrrml_file_path, \"r\", encoding=\"utf-8\") as yarrrml_file:\n",
    "                yarrrml_content = yaml.load(yarrrml_file)\n",
    "            \n",
    "            # Translate YARRRML to RML\n",
    "            rml_output = yatter.translate(yarrrml_content)\n",
    "            \n",
    "            # Replace the RML prefix with the new namespace\n",
    "            rml_output = rml_output.replace(\"http://semweb.mmlab.be/ns/rml#\", \"http://w3id.org/rml/\")\n",
    "            \n",
    "            # Define the output RML filename\n",
    "            rml_filename = filename.replace(\".yaml\", \".rml\")\n",
    "            rml_file_path = os.path.join(rml_files_dir, rml_filename)\n",
    "\n",
    "            # Write the updated RML content to the output file\n",
    "            with open(rml_file_path, \"w\", encoding=\"utf-8\") as rml_file:\n",
    "                rml_file.write(rml_output)\n",
    "\n",
    "            print(f\"RML file created: {rml_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {filename} to RML. Error: {e}\")\n",
    "            traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926778f-15e4-4002-98f0-873e8a0af6c4",
   "metadata": {},
   "source": [
    "### **Creation of RDF Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c3703-b7d2-4d26-8e63-77bc5683dacc",
   "metadata": {},
   "source": [
    "In this section, we will explain how to generate RDF files using the mapping files (in RML format) and the `morph-kgc` library. `morph-kgc` is a Python package that enables us to transform relational data into RDF using the R2RML, RML, or RML-star mappings. It is a powerful tool that automates the process of converting structured data into RDF triples, which are then serialized into a format such as Turtle, N-Triples, or RDF/XML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b51732-a5e7-4b7d-90fb-7afa3eccb46d",
   "metadata": {},
   "source": [
    "**Step 1: Create the Configuration File for `morph-kgc`**\n",
    "\n",
    "Before executing the mapping, we need to create a configuration file that `morph-kgc` will use to understand where the data and mappings are located and how to produce the RDF output.\n",
    "\n",
    "Here’s how the configuration file, typically named `config.ini`, should be structured:\n",
    "\n",
    "```ini\n",
    "[DEFAULT]\n",
    "# Path to the input data source(s)\n",
    "# For JSON or CSV files, specify the path to the file(s)\n",
    "# If using a relational database, provide connection details\n",
    "output_format = turtle\n",
    "\n",
    "# Specify the output directory where the RDF triples will be saved\n",
    "output_dir = \n",
    "\n",
    "# (Optional) If working with a relational database, provide connection details\n",
    "# for relational data sources like MySQL, PostgreSQL, etc.\n",
    "[DataSource1]\n",
    "# For JSON/CSV files, skip this section\n",
    "# For relational databases, use the following format:\n",
    "mappings = path/to/mapping_file1.rml.ttl\n",
    "\n",
    "[DataSourceN]\n",
    "# For JSON/CSV files, skip this section\n",
    "# For relational databases, use the following format:\n",
    "mappings = path/to/mapping_fileN.rml.ttl\n",
    "\n",
    "```\n",
    "\n",
    "In this file:\n",
    "- **`mappings`**: This is the path to the RML mapping file that specifies how to transform the data into RDF.\n",
    "- **`output_format`**: This specifies the format of the output RDF file (e.g., `turtle`, `ntriples`, etc.).\n",
    "- **`output_dir`**: This is where the RDF triples will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50449ef6-a452-4930-a76b-88238465417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:55,205 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1022.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:43:55,209 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1022`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1022.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1022.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mapping file: pl_20_500_11752_OPEN_1022.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:56,002 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:43:56,011 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:43:56,017 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:43:56,022 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:43:56,024 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:43:56,027 | INFO: Mappings processed in 0.807 seconds.\n",
      "2024-10-09 15:43:56,032 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:43:56,645 | INFO: Number of triples generated in total: 9560.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1022 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1022/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:57,881 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1018.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:43:57,883 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1018`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1018.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1018.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1022.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1018.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:43:58,621 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:43:58,629 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:43:58,634 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:43:58,639 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:43:58,642 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:43:58,643 | INFO: Mappings processed in 0.756 seconds.\n",
      "2024-10-09 15:43:58,647 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:43:59,056 | INFO: Number of triples generated in total: 1768.\n",
      "2024-10-09 15:43:59,192 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1017.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:43:59,194 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1017`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1017.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1017.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1018 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1018/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1018.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1017.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:00,130 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:00,138 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:00,143 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:00,147 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:00,149 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:00,151 | INFO: Mappings processed in 0.952 seconds.\n",
      "2024-10-09 15:44:00,157 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:00,648 | INFO: Number of triples generated in total: 6452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1017 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1017/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1017.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1014.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:01,129 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1014.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:01,131 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1014`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1014.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1014.rml'}\n",
      "2024-10-09 15:44:02,022 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:02,029 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:02,036 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:02,040 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:02,043 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:02,045 | INFO: Mappings processed in 0.908 seconds.\n",
      "2024-10-09 15:44:02,048 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:02,508 | INFO: Number of triples generated in total: 1477.\n",
      "2024-10-09 15:44:02,681 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1021.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:02,683 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1021`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1021.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1021.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1014 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1014/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1014.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1021.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:03,501 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:03,511 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:03,517 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:03,520 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:03,524 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:03,525 | INFO: Mappings processed in 0.836 seconds.\n",
      "2024-10-09 15:44:03,528 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:04,226 | INFO: Number of triples generated in total: 4337.\n",
      "2024-10-09 15:44:04,542 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1025.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:04,544 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1025`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1025.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1025.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1021 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1021/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1021.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1025.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:05,480 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:05,488 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:05,493 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:05,497 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:05,500 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:05,501 | INFO: Mappings processed in 0.953 seconds.\n",
      "2024-10-09 15:44:05,506 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:06,139 | INFO: Number of triples generated in total: 5768.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1025 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1025/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:06,589 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_975.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:06,591 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_975`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_975.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_975.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1025.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_975.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:07,380 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:07,389 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:07,397 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:07,401 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:07,403 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:07,405 | INFO: Mappings processed in 0.808 seconds.\n",
      "2024-10-09 15:44:07,409 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:07,755 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1016.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:07,761 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1016`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1016.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1016.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_975.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_975.json'\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1016.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:08,873 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:08,881 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:08,888 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:08,893 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:08,895 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:08,896 | INFO: Mappings processed in 1.127 seconds.\n",
      "2024-10-09 15:44:08,900 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:09,411 | INFO: Number of triples generated in total: 6844.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1016 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1016/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1016.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1026.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:09,919 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1026.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:09,921 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1026`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1026.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1026.rml'}\n",
      "2024-10-09 15:44:10,909 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:10,931 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:10,940 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:10,946 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:10,949 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:10,952 | INFO: Mappings processed in 1.025 seconds.\n",
      "2024-10-09 15:44:10,956 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:11,481 | INFO: Number of triples generated in total: 7328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1026 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1026/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:12,042 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1006.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:12,045 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1006`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1006.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1006.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1026.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1006.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:12,969 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:12,977 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:12,982 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:12,987 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:12,990 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:12,993 | INFO: Mappings processed in 0.942 seconds.\n",
      "2024-10-09 15:44:12,997 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:13,319 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_995.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:13,325 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_995`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_995.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_995.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_1006.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_1006.json'\n",
      "Processing mapping file: pl_20_500_11752_OPEN_995.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:14,123 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:14,132 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:14,138 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:14,143 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:14,145 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:14,147 | INFO: Mappings processed in 0.812 seconds.\n",
      "2024-10-09 15:44:14,151 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:14,495 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1008.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:14,498 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1008`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1008.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1008.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_995.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_995.json'\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1008.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:15,290 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:15,298 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:15,305 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:15,310 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:15,312 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:15,314 | INFO: Mappings processed in 0.810 seconds.\n",
      "2024-10-09 15:44:15,318 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:15,640 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_994.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:15,643 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_994`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_994.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_994.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_1008.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_1008.json'\n",
      "Processing mapping file: pl_20_500_11752_OPEN_994.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:16,433 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:16,441 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:16,448 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:16,453 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:16,456 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:16,458 | INFO: Mappings processed in 0.808 seconds.\n",
      "2024-10-09 15:44:16,461 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:16,799 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_987.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:16,803 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_987`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_987.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_987.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_994.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_994.json'\n",
      "Processing mapping file: pl_20_500_11752_OPEN_987.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:17,636 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:17,645 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:17,651 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:17,656 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:17,658 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:17,660 | INFO: Mappings processed in 0.843 seconds.\n",
      "2024-10-09 15:44:17,664 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:17,997 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1027.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:18,000 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1027`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1027.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1027.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_987.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_987.json'\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1027.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:18,757 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:18,765 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:18,771 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:18,776 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:18,779 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:18,781 | INFO: Mappings processed in 0.775 seconds.\n",
      "2024-10-09 15:44:18,784 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:19,260 | INFO: Number of triples generated in total: 5535.\n",
      "2024-10-09 15:44:20,122 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1020.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1027 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1027/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1027.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1020.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:20,124 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1020`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1020.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1020.rml'}\n",
      "2024-10-09 15:44:20,864 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:20,871 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:20,877 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:20,881 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:20,884 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:20,885 | INFO: Mappings processed in 0.757 seconds.\n",
      "2024-10-09 15:44:20,888 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:21,317 | INFO: Number of triples generated in total: 2594.\n",
      "2024-10-09 15:44:21,544 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1019.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:21,546 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1019`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1019.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1019.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1020 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1020/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1020.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1019.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:22,400 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:22,412 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:22,419 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:22,423 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:22,425 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:22,427 | INFO: Mappings processed in 0.876 seconds.\n",
      "2024-10-09 15:44:22,430 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:22,942 | INFO: Number of triples generated in total: 10952.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1019 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1019/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:23,991 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1015.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:23,993 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1015`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1015.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1015.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1019.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1015.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:24,891 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:24,899 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:24,905 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:24,909 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:24,912 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:24,915 | INFO: Mappings processed in 0.917 seconds.\n",
      "2024-10-09 15:44:24,918 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:25,420 | INFO: Number of triples generated in total: 4897.\n",
      "2024-10-09 15:44:25,814 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1024.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:25,816 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_1024`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_1024.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_1024.rml'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1015 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1015/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1015.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_1024.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:26,594 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:26,603 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:26,609 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:26,613 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:26,615 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:26,616 | INFO: Mappings processed in 0.796 seconds.\n",
      "2024-10-09 15:44:26,619 | DEBUG: Parallelizing with 16 cores.\n",
      "2024-10-09 15:44:27,077 | INFO: Number of triples generated in total: 5371.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dynamic prefix: pl_20_500_11752_OPEN_1024 -> https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/pl_20_500_11752_OPEN_1024/\n",
      "RDF file generated successfully: rdf_files/pl_20_500_11752_OPEN_1024.ttl\n",
      "Processing mapping file: pl_20_500_11752_OPEN_993.rml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:44:27,723 | DEBUG: CONFIGURATION: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_993.ttl', 'na_values': ',nan', 'safe_percent_encoding': '', 'read_parsed_mappings_path': '', 'write_parsed_mappings_path': '', 'mapping_partitioning': 'PARTIAL-AGGREGATIONS', 'logging_file': '', 'oracle_client_lib_dir': '', 'oracle_client_config_dir': '', 'udfs': '', 'output_kafka_server': '', 'output_kafka_topic': '', 'output_dir': '', 'only_printable_chars': 'no', 'infer_sql_datatypes': 'no', 'logging_level': 'INFO', 'number_of_processes': '16'}\n",
      "2024-10-09 15:44:27,726 | DEBUG: DATA SOURCE `pl_20_500_11752_OPEN_993`: {'output_format': 'N-TRIPLES', 'output_file': '/home/jovyan/work/REALITER/rdf_files/pl_20_500_11752_OPEN_993.ttl', 'mappings': '/home/jovyan/work/REALITER/rml_files/pl_20_500_11752_OPEN_993.rml'}\n",
      "2024-10-09 15:44:28,473 | INFO: 12 mapping rules retrieved.\n",
      "2024-10-09 15:44:28,481 | DEBUG: All predicate maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:28,486 | DEBUG: All graph maps are constant-valued, invariant subset is not enforced.\n",
      "2024-10-09 15:44:28,490 | INFO: Mapping partition with 11 groups generated.\n",
      "2024-10-09 15:44:28,493 | INFO: Maximum number of rules within mapping group: 2.\n",
      "2024-10-09 15:44:28,495 | INFO: Mappings processed in 0.764 seconds.\n",
      "2024-10-09 15:44:28,498 | DEBUG: Parallelizing with 16 cores.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing the RML file pl_20_500_11752_OPEN_993.rml: [Errno 2] No such file or directory: '/home/jovyan/work/REALITER/input_data/20_500_11752_OPEN_993.json'\n"
     ]
    }
   ],
   "source": [
    "# Definisci i prefissi predefiniti\n",
    "DEFAULT_PREFIXES = {\n",
    "    \"dc\": \"http://purl.org/dc/elements/1.1/\",\n",
    "    \"dct\": \"http://purl.org/dc/terms/\",\n",
    "    \"iso369-3\": \"http://iso639-3.sil.org/code/\",\n",
    "    \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n",
    "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\n",
    "}\n",
    "\n",
    "def add_prefixes_to_graph(graph, mapping_name):\n",
    "    \"\"\"Add default and dynamic prefixes to the RDFLib graph.\"\"\"\n",
    "    # Aggiungi i prefissi predefiniti\n",
    "    for prefix, namespace in DEFAULT_PREFIXES.items():\n",
    "        graph.bind(prefix, Namespace(namespace))\n",
    "    \n",
    "    # Aggiungi il prefisso dinamico basato sul nome del file\n",
    "    dynamic_prefix = f\"https://vocabs.ilc4clarin.ilc.cnr.it/vocabularies/{mapping_name}/\"\n",
    "    graph.bind(mapping_name, Namespace(dynamic_prefix))\n",
    "    print(f\"Added dynamic prefix: {mapping_name} -> {dynamic_prefix}\")\n",
    "\n",
    "def create_and_process_config_string(output_dir, mapping_files_dir):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Iterate over each mapping file in the mapping_files_dir\n",
    "    for mapping_file in os.listdir(mapping_files_dir):\n",
    "        if mapping_file.endswith(\".rml\") or mapping_file.endswith(\".rml.ttl\"):\n",
    "            # Define output file name\n",
    "            mapping_name = os.path.splitext(mapping_file)[0]\n",
    "            mapping_path = os.path.abspath(os.path.join(mapping_files_dir, mapping_file))\n",
    "            output_file = f\"{mapping_name}.ttl\"\n",
    "            \n",
    "            # Build the configuration string\n",
    "            config_string = f\"\"\"\n",
    "            [DEFAULT]\n",
    "            output_format = N-TRIPLES\n",
    "            output_file = {os.path.join(os.path.abspath(output_dir), output_file)}\n",
    "\n",
    "            [{mapping_name}]\n",
    "            mappings = {mapping_path}\n",
    "            \"\"\"\n",
    "\n",
    "            print(f\"Processing mapping file: {mapping_file}\")\n",
    "            try:\n",
    "                # Use morph-kgc as a library to generate RDF triples using config string\n",
    "                graph = morph_kgc.materialize(config_string)\n",
    "                \n",
    "                # Add default and dynamic prefixes to the graph\n",
    "                add_prefixes_to_graph(graph, mapping_name)\n",
    "\n",
    "                # Serialize the RDFLib graph to a Turtle file\n",
    "                output_ttl_path = os.path.join(output_dir, output_file)\n",
    "                graph.serialize(destination=output_ttl_path, format='turtle')\n",
    "                print(f\"RDF file generated successfully: {output_ttl_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing the RML file {mapping_file}: {str(e)}\")\n",
    "\n",
    "# Define the directories\n",
    "output_directory = \"rdf_files\"\n",
    "rml_files_directory = \"rml_files\"\n",
    "\n",
    "# Check if mapping files directory exists and create config files for each RML file\n",
    "if not os.path.exists(rml_files_directory):\n",
    "    print(f\"Mapping files directory '{rml_files_directory}' does not exist.\")\n",
    "else:\n",
    "    # Create the config files and process each RML file\n",
    "    create_and_process_config_string(output_directory, rml_files_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed630f4f-87a1-4fa3-8f4d-8adca9445b01",
   "metadata": {},
   "source": [
    "**Step 2: Aggregating Concepts to ConceptScheme**\n",
    "\n",
    "After generating the `.ttl` RDF files containing both `skos:ConceptScheme` and `skos:Concept` resources, we need to establish the relationships between the `skos:ConceptScheme` and its top-level concepts. This step ensures the correct linkage between the concepts and their overarching schema.\n",
    "\n",
    "There are two key properties we will use:\n",
    "1. **`skos:hasTopConcept`**: This property will connect the `skos:ConceptScheme` to its top-level `skos:Concept`.\n",
    "2. **`skos:topConceptOf`**: This is the inverse property, which links the `skos:Concept` back to its `skos:ConceptScheme`.\n",
    "\n",
    "The goal is to:\n",
    "- Add `skos:hasTopConcept` for each concept within the concept scheme.\n",
    "- Add `skos:topConceptOf` for each concept to reference the parent concept scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cb6c5d1-26f0-46a7-a4d5-8b3e1a6cf65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated rdf_files/pl_20_500_11752_OPEN_1018.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1026.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1025.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1016.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1024.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1014.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1019.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1017.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1021.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1020.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1027.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1022.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n",
      "Updated rdf_files/pl_20_500_11752_OPEN_1015.ttl with skos:hasTopConcept and skos:topConceptOf relationships\n"
     ]
    }
   ],
   "source": [
    "# Define the SKOS namespace\n",
    "SKOS_NS = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "\n",
    "def aggregate_concepts_to_scheme(rdf_dir):\n",
    "    \"\"\"\n",
    "    Aggregate concepts to concept schemes by adding skos:hasTopConcept and skos:topConceptOf properties.\n",
    "    This updates the existing TTL files.\n",
    "    \"\"\"\n",
    "    for rdf_file in os.listdir(rdf_dir):\n",
    "        if rdf_file.endswith(\".ttl\"):\n",
    "            rdf_file_path = os.path.join(rdf_dir, rdf_file)\n",
    "            graph = Graph()\n",
    "            graph.parse(rdf_file_path, format=\"ttl\")\n",
    "\n",
    "            # Find the concept scheme\n",
    "            concept_scheme = None\n",
    "            for scheme in graph.subjects(RDF.type, SKOS.ConceptScheme):\n",
    "                concept_scheme = scheme\n",
    "                break\n",
    "\n",
    "            if concept_scheme is None:\n",
    "                print(f\"No ConceptScheme found in {rdf_file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Find all top concepts (skos:Concept)\n",
    "            concepts = list(graph.subjects(RDF.type, SKOS.Concept))\n",
    "\n",
    "            # Add skos:hasTopConcept and skos:topConceptOf properties\n",
    "            for concept in concepts:\n",
    "                # Link from the scheme to the concept\n",
    "                graph.add((concept_scheme, SKOS.hasTopConcept, concept))\n",
    "                # Link from the concept to the scheme\n",
    "                graph.add((concept, SKOS.topConceptOf, concept_scheme))\n",
    "\n",
    "            # Serialize and update the RDF file\n",
    "            graph.serialize(destination=rdf_file_path, format=\"turtle\")\n",
    "            print(f\"Updated {rdf_file_path} with skos:hasTopConcept and skos:topConceptOf relationships\")\n",
    "\n",
    "# Directory containing RDF Turtle files\n",
    "rdf_directory = \"rdf_files\"\n",
    "\n",
    "# Run the aggregation\n",
    "aggregate_concepts_to_scheme(rdf_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831728ba-707f-4b75-824a-bb45d9e7477d",
   "metadata": {},
   "source": [
    "**Step 3: Normalizing Character Encoding**\n",
    "\n",
    "In this step, we will normalize the character encoding of the `skos:Concept` URIs in the generated `.ttl` files. This process involves checking if any of the `skos:Concept` URIs contain special percent-encoded characters, such as `%C3%A9` for `é`. If such encodings are found, they will be decoded back to their proper UTF-8 characters. This ensures that the URIs remain human-readable and correctly represent accented or special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8acb8bb-deb4-4111-bd00-e0bc6d99462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1018.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1026.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1025.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1016.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1024.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1014.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1019.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1017.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1021.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1020.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1027.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1022.ttl\n",
      "Normalized URIs and saved the updated file: rdf_files/pl_20_500_11752_OPEN_1015.ttl\n"
     ]
    }
   ],
   "source": [
    "def is_percent_encoded(uri):\n",
    "    \"\"\"\n",
    "    Check if the given URI contains percent-encoded characters.\n",
    "    \"\"\"\n",
    "    return '%' in uri\n",
    "\n",
    "def normalize_concept_uris_in_ttl(ttl_file):\n",
    "    \"\"\"\n",
    "    Function to normalize percent-encoded characters in URIs in a Turtle file\n",
    "    while preserving the existing prefixes.\n",
    "    \"\"\"\n",
    "    # Load the Turtle file into an RDFLib graph\n",
    "    g = Graph()\n",
    "    g.parse(ttl_file, format=\"turtle\")\n",
    "\n",
    "    # Create a new graph for the normalized triples\n",
    "    updated_graph = Graph()\n",
    "\n",
    "    # Copy namespaces (prefixes) from the original graph to the new graph\n",
    "    for prefix, namespace in g.namespace_manager.namespaces():\n",
    "        updated_graph.bind(prefix, namespace)\n",
    "\n",
    "    # Iterate over triples in the original graph\n",
    "    for subj, pred, obj in g:\n",
    "        subj_str = str(subj)\n",
    "        obj_str = str(obj)\n",
    "\n",
    "        # Check and normalize percent-encoded characters in the subject URI\n",
    "        if is_percent_encoded(subj_str):\n",
    "            cleaned_subj = urllib.parse.unquote(subj_str)\n",
    "            subj = URIRef(cleaned_subj)  # Ensure it's an RDFLib URIRef\n",
    "\n",
    "        # Check and normalize percent-encoded characters in the object URI if it is a URIRef\n",
    "        if isinstance(obj, URIRef) and is_percent_encoded(obj_str):\n",
    "            cleaned_obj = urllib.parse.unquote(obj_str)\n",
    "            obj = URIRef(cleaned_obj)  # Ensure it's an RDFLib URIRef\n",
    "\n",
    "        # Add the updated triples to the new graph\n",
    "        updated_graph.add((subj, pred, obj))\n",
    "    \n",
    "    # Write the normalized graph back to the Turtle file with UTF-8 encoding, preserving prefixes\n",
    "    with open(ttl_file, \"wb\") as f:\n",
    "        updated_graph.serialize(destination=f, format='turtle', encoding='utf-8')\n",
    "\n",
    "    print(f\"Normalized URIs and saved the updated file: {ttl_file}\")\n",
    "\n",
    "def normalize_all_ttl_files(directory):\n",
    "    \"\"\"\n",
    "    Function to normalize URIs in all Turtle files in a given directory\n",
    "    while preserving the prefixes in each file.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".ttl\"):\n",
    "            ttl_file_path = os.path.join(directory, filename)\n",
    "            normalize_concept_uris_in_ttl(ttl_file_path)\n",
    "\n",
    "# Specify the directory containing the .ttl files\n",
    "ttl_files_directory = \"rdf_files\"\n",
    "\n",
    "# Run normalization on all .ttl files in the directory\n",
    "normalize_all_ttl_files(ttl_files_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
